{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v1Y8HvD7uRi",
        "outputId": "213701b7-6964-4d86-dcfc-5f5b0042fc59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training::::\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "100%|██████████| 750/750 [04:36<00:00,  2.71it/s]\n",
            "100%|██████████| 250/250 [00:30<00:00,  8.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5978758344252905 , Eval Loss: 0.4227723827064037 , Accuracy: 0.8075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 750/750 [04:35<00:00,  2.72it/s]\n",
            "100%|██████████| 250/250 [00:30<00:00,  8.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.3566832172026237 , Eval Loss: 0.3420039056092501 , Accuracy: 0.852\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "import pandas as pd\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm \n",
        "import numpy as np\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Classe de dataset pour gérer les données textuelles de l'IMDB pour l'entraînement de modèles BERT.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, device, model_name_or_path=\"bert-base-uncased\", max_length=250):\n",
        "        \"\"\"\n",
        "        Initialise le dataset avec les données du fichier CSV, le tokenizer BERT et le device (CPU ou GPU).\n",
        "\n",
        "        Args:\n",
        "            csv_file (str): Chemin du fichier CSV contenant les données.\n",
        "            device (torch.device): Device pour le traitement des tensors.\n",
        "            model_name_or_path (str): Nom ou chemin du modèle BERT pré-entraîné.\n",
        "            max_length (int): Longueur maximale des séquences après tokenisation.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.labels = self.df.sentiment.unique()\n",
        "        labels_dict = {l: indx for indx, l in enumerate(self.labels)}\n",
        "        self.df[\"sentiment\"] = self.df[\"sentiment\"].map(labels_dict)\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path) # On instancie également le tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Retourne la taille du dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Nombre total d'exemples dans le dataset.\n",
        "        \"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "       on va parcourir par index et prendre l'élement corrspondant. l'element et le  text qui sera passer dans le tockenizee\n",
        "\n",
        "        Args:\n",
        "            index (int): Indice de l'exemple à récupérer.\n",
        "\n",
        "        Returns:\n",
        "            dict: Un dictionnaire contenant les input_ids, attention_mask et le label.\n",
        "        \"\"\"\n",
        "        review_text = str(self.df.review[index])# on recupère les text et les index correspondants\n",
        "        label_review = self.df.sentiment[index] # on recupère les 7 classes  et les index correspondants\n",
        "        inputs = self.tokenizer(review_text, padding=\"max_length\", max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
        "        #  on a notre imnput et un effectue un  padding\n",
        "        labels = torch.tensor(label_review) # on convertir les donner numpy en pytor. on a les labels\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0).to(self.device),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0).to(self.device),\n",
        "            \"labels\": labels.to(self.device)\n",
        "        }\n",
        "\n",
        "## fin castumer dataset qui va lire notre fichier csv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CustomBert(nn.Module):\n",
        "    \"\"\"\n",
        "    CustomBert est une classe de modèle utilisant BERT pour la classification de texte.\n",
        "    Elle intègre un modèle BERT pré-entraîné et ajoute une couche de classification linéaire.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name_or_path=\"bert-base-uncased\", n_classes=2): ## nous avons 2 classe ou catégorie de classe\n",
        "        \"\"\"\n",
        "        Initialise le modèle CustomBert avec un modèle BERT pré-entraîné et une couche de classification.\n",
        "\n",
        "        Args:\n",
        "            model_name_or_path (str): Nom ou chemin du modèle BERT pré-entraîné.\n",
        "            n_classes (int): Nombre de classes pour la tâche de classification.\n",
        "        \"\"\"\n",
        "        super(CustomBert, self).__init__()\n",
        "        self.bert_pretrained = BertModel.from_pretrained(model_name_or_path)\n",
        "        self.classifier = nn.Linear(self.bert_pretrained.config.hidden_size, n_classes)\n",
        "        # # On veut faire une classification multiclase avec une couche linéaire\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Passe les entrées par le modèle BERT et la couche de classification.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Les IDs d'entrée des tokens.\n",
        "            attention_mask (torch.Tensor): Les masques d'attention.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Les scores de classification pour chaque classe.\n",
        "        \"\"\"\n",
        "        x = self.bert_pretrained(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.classifier(x.pooler_output)\n",
        "        return x\n",
        "\n",
        "  #def save_check_pointst(self, path):\n",
        "  #  torch.save(self.tate_dic(), path)\n",
        "\n",
        "def training_step(model, data_loader, loss_fn, optimiser):\n",
        "    \"\"\"\n",
        "    Exécute une étape d'entraînement pour une époque complète.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Le modèle à entraîner.\n",
        "        data_loader (DataLoader): Le DataLoader pour les données d'entraînement.\n",
        "        loss_fn (nn.Module): La fonction de perte.\n",
        "        optimiser (Optimizer): L'optimiseur pour mettre à jour les poids du modèle.\n",
        "\n",
        "    Returns:\n",
        "        float: La perte moyenne sur l'époque.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in tqdm(data_loader, total=len(data_loader)):\n",
        "        input_ids = data[\"input_ids\"]\n",
        "        attention_mask = data[\"attention_mask\"]\n",
        "        labels = data[\"labels\"]\n",
        "\n",
        "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(output, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimiser.step()\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)# *100 multiplier par 100\n",
        "\n",
        "def evaluation(model, test_dataloader, loss_fn):\n",
        "    \"\"\"\n",
        "    Évalue le modèle sur les données de test.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Le modèle à évaluer.\n",
        "        test_dataloader (DataLoader): Le DataLoader pour les données de test.\n",
        "        loss_fn (nn.Module): La fonction de perte.\n",
        "\n",
        "    Returns:\n",
        "        tuple: La perte moyenne et l'exactitude sur le dataset de test.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    losses = [] # a chaue fois que je fait une prédiction je recupère  la loss\n",
        "\n",
        "    for data in tqdm(test_dataloader, total=len(test_dataloader)):\n",
        "        input_ids = data[\"input_ids\"]\n",
        "        attention_mask = data[\"attention_mask\"]\n",
        "        labels = data[\"labels\"]\n",
        "\n",
        "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, pred = output.max(1) #   je prend la classe prédict\n",
        "\n",
        "        correct_predictions += torch.sum(pred == labels) # à chaque fois qu'on fait une prédiction on compare au labels\n",
        "        loss = loss_fn(output, labels) # on calcule la loss\n",
        "        losses.append(loss.item()) # on stock tous les loss dans notre liste \"losses\" grace à la methode append()\n",
        "\n",
        "    return np.mean(losses), correct_predictions.double() / len(test_dataloader.dataset) # on calcul la moyenne de la loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Fonction principale pour exécuter l'entraînement et l'évaluation du modèle.\n",
        "    \"\"\"\n",
        "    print(\"Training::::\")\n",
        "\n",
        "    # Hyperparamètres et configuration\n",
        "    N_EPOCHS =2\n",
        "    LR = 2e-5\n",
        "    BATCH_SIZE =8\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Chargement des datasets d'entraînement et de test\n",
        "    train_dataset = IMDBDataset(csv_file=\"/content/train_dataset_allocine_french.csv\", device=device, max_length=250)\n",
        "    test_dataset  = IMDBDataset(csv_file=\"/content/test_dataset_allocine_french.csv\", device=device, max_length=250)\n",
        "\n",
        "    # Création des DataLoaders\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
        "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialisation du modèle, de la fonction de perte et de l'optimiseur\n",
        "    model = CustomBert()\n",
        "    model = model.to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    # Boucle d'entraînement et d'évaluation\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        loss_train = training_step(model, train_dataloader, loss_fn, optimizer)\n",
        "        loss_eval, accuracy = evaluation(model, test_dataloader, loss_fn)\n",
        "\n",
        "        print(f\"Train Loss: {loss_train} , Eval Loss: {loss_eval} , Accuracy: {accuracy}\")\n",
        "\n",
        "    # Sauvegarde  du modèle\n",
        "    torch.save(model.state_dict(), \"my_custom_bert_allocine.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "KxswYRTBAm-c",
        "outputId": "597c5ca6-cf58-4f71-a99e-2c9b6ca5c0cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-3ce45ab12ca6>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"my_custom_bert_allocine.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://4ceb7d0d873957b7f9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4ceb7d0d873957b7f9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gradio as gr\n",
        "\n",
        "# Définition de la classe CustomBert\n",
        "class CustomBert(nn.Module):\n",
        "    def __init__(self, model_name_or_path=\"bert-base-uncased\", n_classes=2):\n",
        "        super(CustomBert, self).__init__()\n",
        "        self.bert_pretrained = BertModel.from_pretrained(model_name_or_path)\n",
        "        self.classifier = nn.Linear(self.bert_pretrained.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.bert_pretrained(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.classifier(x.pooler_output)\n",
        "        return x\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = CustomBert()\n",
        "model.load_state_dict(torch.load(\"my_custom_bert_allocine.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Définition de la fonction de classification\n",
        "def classifier_fn(texte: str):\n",
        "    labels = {0:'positive', 1:'negative'}\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    inputs = tokenizer(texte, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "\n",
        "    _, pred = output.max(1)\n",
        "    return labels[pred.item()]\n",
        "\n",
        "# Configuration et lancement de l'interface Gradio\n",
        "demo = gr.Interface(\n",
        "    fn=classifier_fn,\n",
        "    inputs=[\"text\"],\n",
        "    outputs=[\"text\"],\n",
        ")\n",
        "\n",
        "demo.launch()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb3dJcZy6ktN",
        "outputId": "4ae8c1f2-2ce2-45b0-bcd9-fa5ce25dfb7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.5 semantic-version-2.10.0 starlette-0.38.5 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "! pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZovffSc1IxA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
